{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n\n# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections\nimport warnings\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.linear_model import LogisticRegression\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# 데이터 불러오기\ndf = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')\n\n#모델 함수\ndef modeling(model,X_train,X_test,y_train,y_test):\n    model.fit(X_train,y_train)\n    pred = model.predict(X_test)\n    metrics(y_test,pred)\n\n#평가 지표\ndef metrics(y_test,pred):\n    accuracy = accuracy_score(y_test,pred)\n    precision = precision_score(y_test,pred)\n    recall = recall_score(y_test,pred)\n    f1 = f1_score(y_test,pred)\n    roc_score = roc_auc_score(y_test,pred,average='macro')\n    print('Accuracy : {0:.2f}, Precision : {1:.2f}, Recall : {2:.2f}'.format(accuracy,precision,recall))\n    print('F1-score : {0:.2f}, auc : {1:.2f}'.format(f1,roc_score,recall))\n    \n# Data 특성\ndf.shape # data n: 284807 , p: 31 (Time, Amount, Class외 28개-)\n\n# Data bar plot \ndf.Class.value_counts(normalize=True).plot(kind='bar')\nprint(df.Class.value_counts(normalize=True)*100) # 매우 불균형한 데이터임을 확인 가능\n\n# 어느정도 balanced한 데이터에 대해서 실험해보고자 데이터 불균형 해소 방법 강구\n# 정상건을 Undersampling하는 경우와 사기건을 Oversampling하는 두가지 경우가 있는데, 데이터수가 많을때는 undersampling하는 것이 좋다는 연구 결과에 따라 undersampling 진행\n\n\n#############################\n##### Data Set Generate #####\n#############################\n\n# Raw Data\nx_data = df.loc[:, 'Time' : 'Amount']\ny_data = df.loc[:, 'Class']\nX_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.3, random_state = 42)\n\n# DF1: 사기율 15%, DF2: 사기율 10%, DF3: 사기율 5%, DF4: 사기율 2%\n# Undersampling 을 통해 Data Set 생성\nfrom imblearn.under_sampling import RandomUnderSampler\nfraud_class_len=len(df[df['Class']==1])\nnonfraud_class_index=df[df['Class']==0].index\nrandom_nonfraud_index=np.random.choice(nonfraud_class_index,24108, replace=False)\nfraud_class_index = df[df['Class']==1].index\n\n# DF1 Data Set 생성 (사기율 15%)\nDF1_random_nonfraud_index=np.random.choice(nonfraud_class_index,2788, replace=False)\nDF1_index = np.concatenate([fraud_class_index, DF1_random_nonfraud_index])\nDF1_under_sample=df.loc[DF1_index]\n\nsns.countplot(x='Class', data=DF1_under_sample)\n\n# DF2 sample 만드는 과정 (사기율 10%)\nDF2_random_nonfraud_index=np.random.choice(nonfraud_class_index,4428, replace=False)\nDF2_index = np.concatenate([fraud_class_index, DF2_random_nonfraud_index])\nDF2_under_sample=df.loc[DF2_index]\n\nsns.countplot(x='Class', data=DF1_under_sample)\n\n# DF3 sample 만드는 과정 (사기율 5%)\nDF3_random_nonfraud_index=np.random.choice(nonfraud_class_index,9348, replace=False)\nDF3_index = np.concatenate([fraud_class_index, DF3_random_nonfraud_index])\nDF3_under_sample=df.loc[DF2_index]\n\nsns.countplot(x='Class', data=DF3_under_sample)\n\n\n# DF4 sample 만드는 과정 (사기율 2%)\nDF4_random_nonfraud_index=np.random.choice(nonfraud_class_index,24108, replace=False)\nDF4_index = np.concatenate([fraud_class_index, DF4_random_nonfraud_index])\nDF4_under_sample=df.loc[DF4_index]\n\nsns.countplot(x='Class', data=DF4_under_sample)\n\n#########################\n##### Model Fitting #####\n#########################\n\n\n#################\n## 0. Raw data ##\n#################\n## 0-1. Logistic Regression\nLR = LogisticRegression()\nprint('Logistic Regression for Raw data')\nmodeling(LR,X_train,X_test,y_train,y_test) \n\n## 0-2. SVM\nprint('SVM for Raw data')\nsvm_clf =SVC(kernel = 'linear')\nmodeling(svm_clf,X_train,X_test,y_train,y_test)\n\n## 0-3. Random Forest\nprint('Random Forest for Raw data')\nmodel_rf = RandomForestClassifier(n_estimators = 15)\nmodeling(model_rf,X_train,X_test,y_train,y_test)\n\n#################\n## 4. DF1 data ##\n#################\n## 0-1. Logistic Regression\nDF1_x_data = DF1_under_sample.loc[:, 'Time' : 'Amount']\nDF1_y_data = DF1_under_sample.loc[:, 'Class']\nDF1_X_train, DF1_X_test, DF1_y_train, DF1_y_test = train_test_split(DF1_x_data, DF1_y_data, test_size = 0.3, random_state = 42)\nprint('DF1 Logistic Regression') \nmodeling(LR,DF1_X_train, DF1_X_test, DF1_y_train, DF1_y_test)\n\n## 0-2. SVM\n# SVM, kernel = 'linear'로 선형분리 진행\nprint('DF1 SVM') \nsvm_clf =SVC(kernel = 'linear')\nmodeling(svm_clf,DF1_X_train, DF1_X_test, DF1_y_train, DF1_y_test )\n\n## 0-3. Random Forest\nmodel_rf = RandomForestClassifier(n_estimators = 15)\nprint('DF1 Random Forest') \nmodeling(model_rf,DF1_X_train, DF1_X_test, DF1_y_train, DF1_y_test )\n\n\n\n#################\n## 2. DF2 data ##\n#################\n## 0-1. Logistic Regression\nDF2_x_data = DF2_under_sample.loc[:, 'Time' : 'Amount']\nDF2_y_data = DF2_under_sample.loc[:, 'Class']\nDF2_X_train, DF2_X_test, DF2_y_train, DF2_y_test = train_test_split(DF2_x_data, DF2_y_data, test_size = 0.3, random_state = 42)\nprint('DF2 Logistic Regression') \nmodeling(LR,DF2_X_train, DF2_X_test, DF2_y_train, DF2_y_test)\n\n## 0-2. SVM\n# SVM, kernel = 'linear'로 선형분리 진행\nprint('DF2 SVM') \nsvm_clf =SVC(kernel = 'linear')\nmodeling(svm_clf,DF2_X_train, DF2_X_test, DF2_y_train, DF2_y_test)\n\n## 0-3. Random Forest\nmodel_rf = RandomForestClassifier(n_estimators = 15)\nprint('DF2 Random Forest') \nmodeling(model_rf,DF2_X_train, DF2_X_test, DF2_y_train, DF2_y_test)\n\n#################\n## 3. DF3 data ##\n#################\n## 0-1. Logistic Regression\nDF3_x_data = DF3_under_sample.loc[:, 'Time' : 'Amount']\nDF3_y_data = DF3_under_sample.loc[:, 'Class']\nDF3_X_train, DF3_X_test, DF3_y_train, DF3_y_test = train_test_split(DF3_x_data, DF3_y_data, test_size = 0.3, random_state = 42)\nprint('DF3 Logistic Regression') \nmodeling(LR,DF3_X_train, DF3_X_test, DF3_y_train, DF3_y_test)\n\n## 0-2. SVM\n# SVM, kernel = 'linear'로 선형분리 진행\nprint('DF3 SVM') \nsvm_clf =SVC(kernel = 'linear')\nmodeling(svm_clf,DF3_X_train, DF3_X_test, DF3_y_train, DF3_y_test )\n\n## 0-3. Random Forest\nmodel_rf = RandomForestClassifier(n_estimators = 15)\nprint('DF3 Random Forest') \nmodeling(model_rf,DF3_X_train, DF3_X_test, DF3_y_train, DF3_y_test)\n\n\n#################\n## 4. DF4 data ##\n#################\n## 0-1. Logistic Regression\nDF4_x_data = DF4_under_sample.loc[:, 'Time' : 'Amount']\nDF4_y_data = DF4_under_sample.loc[:, 'Class']\nDF4_X_train, DF4_X_test, DF4_y_train, DF4_y_test = train_test_split(DF4_x_data, DF4_y_data, test_size = 0.3, random_state = 42)\nprint('DF4 Logistic Regression') \nmodeling(LR,DF4_X_train, DF4_X_test, DF4_y_train, DF4_y_test)\n\n## 0-2. SVM\n# SVM, kernel = 'linear'로 선형분리 진행\nprint('DF4 SVM') \nsvm_clf =SVC(kernel = 'linear')\nmodeling(svm_clf,DF4_X_train, DF4_X_test, DF4_y_train, DF4_y_test)\n\n## 0-3. Random Forest\nmodel_rf = RandomForestClassifier(n_estimators = 15)\nprint('DF4 Random Forest') \nmodeling(model_rf,DF4_X_train, DF4_X_test, DF4_y_train, DF4_y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}